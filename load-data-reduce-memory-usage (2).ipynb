{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset=sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
       "       'alive', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dataset['survived'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fil_paths=glob.glob(\"C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\application_test.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\application_train.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\bureau.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\bureau_balance.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\credit_card_balance.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\HomeCredit_columns_description.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\installments_payments.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\POS_CASH_balance.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\previous_application.csv',\n",
       " 'C:\\\\Users\\\\ML LABS\\\\Downloads\\\\Datasets\\\\home-credit-default-risk\\\\sample_submission.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fil_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(fil_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.99842834472656"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage().sum()/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 45.00 MB\n"
     ]
    }
   ],
   "source": [
    "start_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-128, 127)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int8).min,np.iinfo(np.int8).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-32768, 32767)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int16).min,np.iinfo(np.int16).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2147483648, 2147483647)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int32).min,np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-9223372036854775808, 9223372036854775807)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int64).min,np.iinfo(np.int64).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 9.40 MB\n",
      "Decreased by 79.1%\n"
     ]
    }
   ],
   "source": [
    "\"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "\"\"\"\n",
    "start_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "for col in df.columns:\n",
    "    col_type = df[col].dtype\n",
    "\n",
    "    if col_type != object:\n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)  \n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df[col] = df[col].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    else:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "end_mem = df.memory_usage().sum() / 1024**2\n",
    "print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac49d985-a0ea-4618-a331-a8609a414321",
    "_uuid": "5204b403ba02dc60f154c8cad9126df107255c54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIP 1 - Deleting unused variables and gc.collect()\n",
    "TIP 2 - Presetting the datatypes\n",
    "TIP 3 - Importing selected rows of the a file (including generating your own subsamples)\n",
    "TIP 4 - Importing in batches and processing each individually\n",
    "TIP 5 - Importing just selected columns\n",
    "TIP 6 - Creative data processing\n",
    "TIP 7 - Using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9eb71f1-aae8-49fe-b59d-50e52f48044b",
    "_uuid": "b5af7f263ddbd7e5a2c7bb0cfc0c49fa12b20348"
   },
   "source": [
    "### How to Work with BIG Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## TIP # 1 Deleting unused variables and gc.collect() \n",
    "\n",
    "The thing about python is that once it loads something into RAM it doesn't really get rid of it effectively.  So if you load a huge dataframe into pandas, and then make a copy of it and never use it again, that original dataframe will still be in your RAM.  Eating away at your memory.   Same goes for any other variables you create.\n",
    "\n",
    "Therefore if you used up a dataframe (or other variable), get in the habit of deleting it.  \n",
    "\n",
    "For example, if you create a dataframe  `temp`, extract some features and merge results to your main training set, `temp` will still be eating up space.  You need to explicitely delete it by stating `del temp`.  You also need to make sure that nothing else is referring to `temp` (you don't have any other variables bound to it).\n",
    "\n",
    "Even after doing so there may still be residual memory usage going on.\n",
    "\n",
    "That's where the garbage collection module comes in.   `import gc` at the beginning of your project, and then each time you want to clear up space put command `gc.collect()` .  \n",
    "\n",
    "It also helps to run `gc.collect()` after multiple transformations/functions/copying etc...  as all the little references/values accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "63cf8776-769f-45e8-bfa6-262168ee64e2",
    "_uuid": "c2dbb1b50a7049b8be5c4cc343dcc389fb80f25d"
   },
   "outputs": [],
   "source": [
    "# eg:\n",
    "#import some file\n",
    "temp = pd.read_csv('../input/train_sample.csv')\n",
    "\n",
    "#do something to the file\n",
    "temp['os'] = temp['os'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
       "       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
       "       'AMT_CREDIT', 'AMT_ANNUITY',\n",
       "       ...\n",
       "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
       "       'FLAG_DOCUMENT_21', 'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       "       'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       "       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT',\n",
       "       'AMT_REQ_CREDIT_BUREAU_YEAR'],\n",
       "      dtype='object', length=122)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do something to the file\n",
    "temp['SK_ID_CURR'] = temp['SK_ID_CURR'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "5367107a-e89f-4d87-8558-896d1704818a",
    "_uuid": "0a03d4eff80f1885b66705637e3bace9fc338264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5128"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete when no longer needed\n",
    "del temp\n",
    "#collect residual garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "19291259-d7a4-435a-8342-0fb6eb2bb7ab",
    "_uuid": "ebc75a51ba874f915d2f309f09a86c5b02390ca7"
   },
   "source": [
    "## TIP # 2   Presetting the datatypes\n",
    "If you import data into CSV, python will do it's best to guess the datatypes, but it will tend to error on the side of allocating more space than necessary.\n",
    "So if you know in advance that your numbers are integers, and don't get bigger than certain values, set the datatypes at minimum requirements before importing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "935e2756-4179-47f7-9de7-9502f31c7236",
    "_uuid": "d2f41f2d57258e6ee0c79696e80c4345fc8dc8b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        }\n",
    "\n",
    "train = pd.read_csv('../input/train_sample.csv', dtype=dtypes)\n",
    "\n",
    "#check datatypes:\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d57f1236-1018-48e1-ac0f-4c7df17826f6",
    "_uuid": "02ab26221208d2dd0ad80520ebc80ebea94ec7e6"
   },
   "source": [
    "## TIP # 3 Importing selected rows of a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79076b96-264c-4e9e-9ac0-213d7cf95045",
    "_uuid": "cb80d6e0ee7e5ad61fbc07d9650d2cc12b1cd002"
   },
   "source": [
    "### a) Select number of rows to import\n",
    "Instead of the default  `pd.read_csv('filename') ` you can use parameter `nrows` to specify number of rows to import.  For exampe:\n",
    "`train = pd.read_csv('../input/train.csv', nrows=10000)` will only read the first 10000 rows (including the heading).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "91adac02-6b53-4c23-9b62-f79d0fd755d2",
    "_uuid": "24e49a60b2a7301c3964dcb8d32576665a35caa9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', nrows=10000, dtype=dtypes)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2b072d2a-5da9-40ac-9d7b-a1de2bde20ea",
    "_uuid": "9b0afc1c942caecfd601ea42231549934d6ab3be"
   },
   "source": [
    "### b)  Simple row skip (with or without headings)\n",
    "You can also specify number of rows to skip (`skiprows`) , if you, for example want 1 million rows after the first 5 million:\n",
    "`train = pd.read_csv('../input/train.csv', skiprows=5000000, nrows=1000000)`.  This however will ignore the first line with headers.  Instead you can pass in range of rows to skip, that will not include the first row  (indexed `[0]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "339c4b5f-603e-47a4-ba62-e8ee67b81aa6",
    "_uuid": "4a3bfba6413f48b1515ca2e037133592388929b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plain skipping looses heading info.  It's OK for files that don't have headings, \n",
    "#or dataframes you'll be linking together, or where you make your own custom headings...\n",
    "train = pd.read_csv('../input/train.csv', skiprows=5000000, nrows=1000000, header = None, dtype=dtypes)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "392e3877-c489-4219-9e89-48d8a59b87f8",
    "_uuid": "ce0057d9030e527655ebee51234b1c949a0a6fbf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#but if you want to import the headings from the original file\n",
    "#skip first 5mil rows, but use the first row for heading:\n",
    "train = pd.read_csv('../input/train.csv', skiprows=range(1, 5000000), nrows=1000000, dtype=dtypes)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2df3c47f-56e9-4260-8399-389022d22438",
    "_uuid": "b591d03631e8c84a9b70afdc39c3bfadeab24564"
   },
   "source": [
    "## TIP #4   Importing in batches and processing each individually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5952b89c-d478-4b7c-9c48-272b4f8ed5b3",
    "_uuid": "7469b61f83cac91aa83b0d742d6722bf86e5a7f2"
   },
   "source": [
    "We know that the proportion of clicks that was attributed is very low.  So let's say we want to look at all of them at the same time.  We don't know what rows they are, and we can't load the whole data and filter.  But we can load in chuncks, extract from each chunk what we need and get rid of everything else!\n",
    "\n",
    "The idea is simple.  You specify size of chunk (number of lines) you want pandas to import at a time.  Then you do some kind of processing on it.  Then pandas imports the next chunk, untill there are no more lines left.\n",
    "\n",
    "So below I import one million rows, extract only rows that have 'is_attributed'==1 (i.e. app was downloaded) and then merge these results into common dataframe for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b694655-096b-49e9-b021-1026c67e5db3",
    "_uuid": "24cb40e94eee91bcad060e151bf93a4e60e15963",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up an empty dataframe\n",
    "df_converted = pd.DataFrame()\n",
    "\n",
    "#we are going to work with chunks of size 1 million rows\n",
    "chunksize = 10 ** 6\n",
    "\n",
    "#in each chunk, filter for values that have 'is_attributed'==1, and merge these values into one dataframe\n",
    "for chunk in pd.read_csv('../input/train.csv', chunksize=chunksize, dtype=dtypes):\n",
    "    filtered = (chunk[(np.where(chunk['is_attributed']==1, True, False))])\n",
    "    df_converted = pd.concat([df_converted, filtered], ignore_index=True, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d2128f49-f618-4fd2-8399-b96308a868e7",
    "_uuid": "cf9dd50755e3d9404da6c4529a0dfdc79d5d75ed"
   },
   "source": [
    "Let's see what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d79eca7-d7f2-49a9-95bc-2278cbbb2c0f",
    "_uuid": "0f2e06b5a1e6e64ce168de2198e21520fff7c622",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_converted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55aa3d09-19b2-4684-9e1a-a57475a0b1f2",
    "_uuid": "5005e936491291e5f3c8aa96361aa0d98e21c7ea",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_converted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c71c3095-293d-43c1-b84a-dff6cf413ab3",
    "_uuid": "7ecec250187bdf751f5d047d97af031d590f57f4"
   },
   "source": [
    "## TIP #5 Importing just selected columns\n",
    "\n",
    "If you want to analyze just some specific feature, you can import just the selected columns.\n",
    "\n",
    "For example, lets say we want to analyze clicks by ips.  Or conversions by ips.\n",
    "\n",
    "Importing just 2 fields as opposed to full table just may fit in our RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7cf6aac7-d1a9-416b-ba6d-a9462fc66c2e",
    "_uuid": "6c3897a04337c56c4bd69ff4c6f32395e3859e04",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wanted columns\n",
    "columns = ['ip', 'click_time', 'is_attributed']\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'is_attributed' : 'uint8',\n",
    "        }\n",
    "\n",
    "ips_df = pd.read_csv('../input/train.csv', usecols=columns, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " df[col]= df[col].astype(np.int8)\n",
    "elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "    df[col]= df[col].astype(np.int16)\n",
    "elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "    df[col]= df[col].astype(np.int32)\n",
    "elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "     df[col]= df[col].astype(np.int64)\n",
    "else:\n",
    "if c_min > np.iinfo(np.float16).min and c_max < np.iinfo(np.float16).max:\n",
    "    df[col]=df[col].astype(np.float16)\n",
    "elif c_min > np.iinfo(np.float32).min and c_max < np.iinfo(np.float32).max:\n",
    "    df[col]=df[col].astype(np.float32)\n",
    "else:\n",
    "    df[col]=df[col].astype(np.float64)\n",
    "else:\n",
    "df[col]=df[col].astype('category')\n",
    "\n",
    "end_mem=df.memory_usage().sum()/1024*1024\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
